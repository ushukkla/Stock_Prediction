{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_stock_trader_custom_environment",
      "provenance": [],
      "collapsed_sections": [
        "2C72oyw5QvFd",
        "R_woJwp4Q_XU",
        "jEBkeuIiRIOM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIE0lafVQyT"
      },
      "source": [
        "# Deep Stock Trader (Advanced) - START HERE\n",
        "*Built for TAMU Datathon 2020 by Seth Hamilton and Josiah Coad.*\n",
        "\n",
        "If you haven't, go ahead and download the zip containing this notebook and more from our [challenges website](https://tamudatathon.com/challenges).\n",
        "\n",
        "Includes usage examples. \n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Predicting stock market performance is a centuries old problem. Ambitious investors have designed and redesigned thousands of trading algorithms, custom indexes, and more to gain an edge in market prediction. Some of the biggest banks in the world hire large numbers of ML experts specifically to improve their investing strategies. Some companies even specialize in this approach (see https://www.twosigma.com/).\n",
        "\n",
        "Reinforcement Learning opens the dooor to a whole new approach to predicting stock prices... in that we don't have to predict prices! Instead, we model trading stocks like a game and train an agent to maximize the reward function we care about: total money gained!\n",
        "\n",
        "---\n",
        "\n",
        "## Problem Description\n",
        "\n",
        "In this challenge, it's your job to train an agent to make a trading decision (buy, sell, hold) to execute at the opening of the following day. You're given an array of values (i.e. open, close, low, high, SMA_10, RSI_14, etc...). If you want, you can write logic to keep track of your current amount of cash and shares. Fractional shares are allowed. \n",
        "\n",
        "In other words. For every row/day in the historical dataset, you'll be given the array:\n",
        "\n",
        "> $[v1, v2, v3, ...]$.\n",
        "\n",
        "The array contains daily historical Your job is to return a tuple containing one of \n",
        "\n",
        "> $Action.BUY, Action.SELL, Action.HOLD$ \n",
        "\n",
        "and a value $frac$ where $frac$ represents the fraction of cash to spend or shares to sell in a trade \n",
        "\n",
        "> $0 \\leq frac \\leq 1$ \n",
        "\n",
        "Thus, your agent's step function should return something like\n",
        "\n",
        "> $(Action.BUY, 0.5)$\n",
        "\n",
        "which can be interpreted as the decision to invest half (0.5) your current amount of cash into the market. Again, you'll return an $(Action, frac)$ tuple for every row/day in the dataset. \n",
        "\n",
        "---\n",
        "\n",
        "## Submission Requirements/Details\n",
        "\n",
        "- Load train.csv into a pandas dataframe\n",
        "- Train a RL agent using our custom gym environment* \n",
        "- Save model to disk \n",
        "- Edit main.py to use model in step function (see sample main.py for details)\n",
        "- Zip main.py and your model together and submit on [tamudatathon.com/koth]\n",
        "- Note your score and try again!\n",
        "\n",
        "*Custom env provided in this notebook and in the util.py. Feel free to modify the env implementation (such as the reward func) to improve performance\n",
        "\n",
        "---\n",
        "\n",
        "## About This Notebook\n",
        "\n",
        "This notebook does several things\n",
        "- **Creates** a custom gym environment to make RL agent training easy\n",
        "- **Validates** and tests the custom gym environment\n",
        "- **Downloads** sample data (not the stock actually used for challenge) and cleans it for use \n",
        "- **Trains** a basic agent to play the trading game \n",
        "- **Tests** the agent to see how much money it makes!\n",
        "\n",
        "You can download this notebook and running it locally on the training dataset so you can train a model for your real submission. \n",
        "\n",
        "One last note, this custom gym environment only accepts a basic BUY, SELL, or HOLD action, not a tuple containing both an action and a fraction. You'll have to modify your final implementation to make use of the fraction feature. (Or don't and simply set fraction = 1). \n",
        "\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C72oyw5QvFd"
      },
      "source": [
        "## Custom Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n41DJ5J4Z3Yl"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZKCS5bVKjy"
      },
      "source": [
        "class DeepStockTraderEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface\n",
        "  This environment enables agents to make a decision at every timestep in\n",
        "  a historical stock environment.\n",
        "\n",
        "  The reward function is defined by how much money the bot made in a particular \n",
        "  timestep. (This is 0 in cases where no shares are held)\n",
        "  \"\"\"\n",
        "\n",
        "  metadata={ 'render.modes': ['console'] }\n",
        "\n",
        "  BUY = 0\n",
        "  SELL = 1\n",
        "  HOLD = 2\n",
        "\n",
        "  def __init__(self, pd_data):\n",
        "    super(DeepStockTraderEnv, self).__init__()\n",
        "\n",
        "    self.data = pd_data.values\n",
        "    self.columns_map = {c.lower(): i for i, c in enumerate(pd_data.columns)}\n",
        "\n",
        "    self.row_size = len(self.columns_map)\n",
        "\n",
        "    min_val = np.min(self.data)\n",
        "    low = np.array([min_val for i in range(self.row_size)])\n",
        "\n",
        "    max_val = np.max(self.data)\n",
        "    high = np.array([max_val for i in range(self.row_size)])\n",
        "\n",
        "    self.observation_space = spaces.Box(low=low, \n",
        "                                            high=high, \n",
        "                                            shape=(self.row_size,), \n",
        "                                            dtype=np.float64)\n",
        "\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "\n",
        "    # Variables that track the bot's current state\n",
        "    self.n_shares = 0 # num of shares currently held\n",
        "    self.cash = 1000  # starting cash\n",
        "    self.timestep = 0 # cur index of row/timestep in dataset\n",
        "    self.n_buys = 0   # num of buys\n",
        "    self.n_sells = 0  # num of sells\n",
        "    self.n_holds = 0  # num of holds\n",
        "    self.account_vals = [] # list tracking the account performance over time\n",
        "\n",
        "  def reset(self):\n",
        "    self.n_shares = 0 \n",
        "    self.cash = 1000\n",
        "    self.timestep = 1 # + 1 since we return the first observation\n",
        "    self.n_buys = 0\n",
        "    self.n_sells = 0\n",
        "    self.n_holds = 0\n",
        "    self.account_vals = []\n",
        "\n",
        "    return np.copy(self.data[0])\n",
        "\n",
        "  def total(self, timestep=-1, open=True):\n",
        "    return self.cash + self.n_shares * self.data[timestep, self.columns_map[\"open\" if open else \"close\"]]\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # ********************** EXECUTE ACTION **********************\n",
        "    open_j = self.columns_map[\"open\"]\n",
        "    close_j = self.columns_map[\"close\"]\n",
        "    if action == self.BUY:\n",
        "        self.n_shares += self.cash / self.data[self.timestep, open_j]\n",
        "        self.cash = 0\n",
        "        self.n_buys += 1\n",
        "    elif action == self.SELL:\n",
        "        self.cash += self.n_shares * self.data[self.timestep, open_j]\n",
        "        self.n_shares = 0\n",
        "        self.n_sells += 1\n",
        "    elif action == self.HOLD:\n",
        "        self.n_holds += 1\n",
        "    else:\n",
        "        raise ValueError(f\"Illegal Action value: {action}\")\n",
        "\n",
        "    self.account_vals.append(self.total(self.timestep))\n",
        "    # ************************************************************\n",
        "\n",
        "    # IMPORTANT \n",
        "    # We define reward to be (total account value at close) - (total account value at open)\n",
        "    # Basically your reward is the amount gained over the course of the day \n",
        "    reward = self.total(self.timestep, open=False) - self.total(self.timestep)\n",
        "    done = self.timestep+1 == len(self.data)-1\n",
        "    info = {\n",
        "        \"n_buys\": self.n_buys,\n",
        "        \"n_sells\": self.n_sells,\n",
        "        \"n_holds\": self.n_holds,\n",
        "        \"cash\": self.cash,\n",
        "        \"n_shares\": self.n_shares\n",
        "    }\n",
        "\n",
        "    self.timestep += 1\n",
        "\n",
        "    return np.copy(self.data[self.timestep]), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    print(f\"------------Step {self.timestep}------------\")\n",
        "    print(f'total:   \\t{self.total(self.timestep)}')\n",
        "    print(f'cash:    \\t{self.cash}')\n",
        "    print(f'n_shares:\\t{self.n_shares}')\n",
        "    print(f'n_buys:  \\t{self.n_buys}')\n",
        "    print(f'n_sells:\\t{self.n_sells}')\n",
        "    print(f'n_holds:\\t{self.n_holds}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_woJwp4Q_XU"
      },
      "source": [
        "## Data Collection and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1kVKTe2MZP1",
        "outputId": "14e50a71-722b-402a-feaf-154f748d3cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install yfinance\n",
        "!pip install pandas-ta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2La5Q1D5x0sD",
        "outputId": "81632161-99c5-42b8-8adc-0e57c892e449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEXOOvvCzWgf",
        "outputId": "1af70da1-8a11-4158-e26e-d4f8e15af498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "import pandas_ta as pdt\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# GET STOCK DATA\n",
        "stonk = yf.Ticker('CANF')\n",
        "df = stonk.history(start=datetime.now() - timedelta(days=2000), end=datetime.now())\n",
        "df.ta.strategy(\"all\")\n",
        "\n",
        "# Clean data\n",
        "percent_missing = df.isnull().sum() * 100 / len(df)\n",
        "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
        "                                 'percent_missing': percent_missing})\n",
        "for row in missing_value_df.iterrows():\n",
        "  if row[1].percent_missing > 0.1:\n",
        "    df.drop(columns=[row[0]], inplace=True)\n",
        "df = df.dropna()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEBkeuIiRIOM"
      },
      "source": [
        "## Env Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exUuAKQex1iG"
      },
      "source": [
        "env = DeepStockTraderEnv(df)\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXjAo6kYCidU",
        "outputId": "40dcc079-75f1-4bb7-bb1e-068ae755139b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "BUY = 0\n",
        "SELL = 1\n",
        "HOLD = 2\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(random.randint(0, 2))\n",
        "  # print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "env.reset();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlWOn0vsRNoV"
      },
      "source": [
        "## Sample Training Loop\n",
        "\n",
        "*See trainer.py for a pytorch example built by Seth Hamilton*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPF3Vp7zMcGY"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = DeepStockTraderEnv(df)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7c67bqQoip",
        "outputId": "75f80276-ab6b-4bc3-efc3-2dd31ed1b7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the agent\n",
        "model = DQN('MlpPolicy', env, verbose=1).learn(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQj1y20MTqT6",
        "outputId": "9a4779e4-a3cd-4cb5-dd5c-c21e9b5faee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "timestep = 1\n",
        "while True:\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "\n",
        "  # if env.total(timestep) > 10000:\n",
        "  #   pdb.set_trace()\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "  timestep += 1\n",
        "env.render(mode='console')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jteD_gijlnb",
        "outputId": "7d447f0d-2463-4238-8153-ba982a1e4241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "big_gain = np.exp(np.log(1908.20/1000)/(2000/365))\n",
        "big_gain"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}