{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_stock_trader_custom_environment",
      "provenance": [],
      "collapsed_sections": [
        "2C72oyw5QvFd",
        "R_woJwp4Q_XU",
        "jEBkeuIiRIOM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLIE0lafVQyT"
      },
      "source": [
        "## Submission Requirements/Details\n",
        "\n",
        "- Load train.csv into a pandas dataframe\n",
        "- Train a RL agent using our custom gym environment* \n",
        "- Save model to disk \n",
        "- Edit main.py to use model in step function (see sample main.py for details)\n",
        "- Zip main.py and your model together and submit on [tamudatathon.com/koth]\n",
        "- Note your score and try again!\n",
        "\n",
        "*Custom env provided in this notebook and in the util.py. Feel free to modify the env implementation (such as the reward func) to improve performance\n",
        "\n",
        "---\n",
        "\n",
        "## About This Notebook\n",
        "\n",
        "This notebook does several things\n",
        "- **Creates** a custom gym environment to make RL agent training easy\n",
        "- **Validates** and tests the custom gym environment\n",
        "- **Downloads** sample data (not the stock actually used for challenge) and cleans it for use \n",
        "- **Trains** a basic agent to play the trading game \n",
        "- **Tests** the agent to see how much money it makes!\n",
        "\n",
        "You can download this notebook and running it locally on the training dataset so you can train a model for your real submission. \n",
        "\n",
        "One last note, this custom gym environment only accepts a basic BUY, SELL, or HOLD action, not a tuple containing both an action and a fraction. You'll have to modify your final implementation to make use of the fraction feature. (Or don't and simply set fraction = 1). \n",
        "\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C72oyw5QvFd"
      },
      "source": [
        "## Custom Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n41DJ5J4Z3Yl"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZKCS5bVKjy"
      },
      "source": [
        "class DeepStockTraderEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface\n",
        "  This environment enables agents to make a decision at every timestep in\n",
        "  a historical stock environment.\n",
        "\n",
        "  The reward function is defined by how much money the bot made in a particular \n",
        "  timestep. (This is 0 in cases where no shares are held)\n",
        "  \"\"\"\n",
        "\n",
        "  metadata={ 'render.modes': ['console'] }\n",
        "\n",
        "  BUY = 0\n",
        "  SELL = 1\n",
        "  HOLD = 2\n",
        "\n",
        "  def __init__(self, pd_data):\n",
        "    super(DeepStockTraderEnv, self).__init__()\n",
        "\n",
        "    self.data = pd_data.values\n",
        "    self.columns_map = {c.lower(): i for i, c in enumerate(pd_data.columns)}\n",
        "\n",
        "    self.row_size = len(self.columns_map)\n",
        "\n",
        "    min_val = np.min(self.data)\n",
        "    low = np.array([min_val for i in range(self.row_size)])\n",
        "\n",
        "    max_val = np.max(self.data)\n",
        "    high = np.array([max_val for i in range(self.row_size)])\n",
        "\n",
        "    self.observation_space = spaces.Box(low=low, \n",
        "                                            high=high, \n",
        "                                            shape=(self.row_size,), \n",
        "                                            dtype=np.float64)\n",
        "\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "\n",
        "    # Variables that track the bot's current state\n",
        "    self.n_shares = 0 # num of shares currently held\n",
        "    self.cash = 1000  # starting cash\n",
        "    self.timestep = 0 # cur index of row/timestep in dataset\n",
        "    self.n_buys = 0   # num of buys\n",
        "    self.n_sells = 0  # num of sells\n",
        "    self.n_holds = 0  # num of holds\n",
        "    self.account_vals = [] # list tracking the account performance over time\n",
        "\n",
        "  def reset(self):\n",
        "    self.n_shares = 0 \n",
        "    self.cash = 1000\n",
        "    self.timestep = 1 # + 1 since we return the first observation\n",
        "    self.n_buys = 0\n",
        "    self.n_sells = 0\n",
        "    self.n_holds = 0\n",
        "    self.account_vals = []\n",
        "\n",
        "    return np.copy(self.data[0])\n",
        "\n",
        "  def total(self, timestep=-1, open=True):\n",
        "    return self.cash + self.n_shares * self.data[timestep, self.columns_map[\"open\" if open else \"close\"]]\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    # ********************** EXECUTE ACTION **********************\n",
        "    open_j = self.columns_map[\"open\"]\n",
        "    close_j = self.columns_map[\"close\"]\n",
        "    if action == self.BUY:\n",
        "        self.n_shares += self.cash / self.data[self.timestep, open_j]\n",
        "        self.cash = 0\n",
        "        self.n_buys += 1\n",
        "    elif action == self.SELL:\n",
        "        self.cash += self.n_shares * self.data[self.timestep, open_j]\n",
        "        self.n_shares = 0\n",
        "        self.n_sells += 1\n",
        "    elif action == self.HOLD:\n",
        "        self.n_holds += 1\n",
        "    else:\n",
        "        raise ValueError(f\"Illegal Action value: {action}\")\n",
        "\n",
        "    self.account_vals.append(self.total(self.timestep))\n",
        "    # ************************************************************\n",
        "\n",
        "    # IMPORTANT \n",
        "    # We define reward to be (total account value at close) - (total account value at open)\n",
        "    # Basically your reward is the amount gained over the course of the day \n",
        "    reward = self.total(self.timestep, open=False) - self.total(self.timestep)\n",
        "    done = self.timestep+1 == len(self.data)-1\n",
        "    info = {\n",
        "        \"n_buys\": self.n_buys,\n",
        "        \"n_sells\": self.n_sells,\n",
        "        \"n_holds\": self.n_holds,\n",
        "        \"cash\": self.cash,\n",
        "        \"n_shares\": self.n_shares\n",
        "    }\n",
        "\n",
        "    self.timestep += 1\n",
        "\n",
        "    return np.copy(self.data[self.timestep]), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    print(f\"------------Step {self.timestep}------------\")\n",
        "    print(f'total:   \\t{self.total(self.timestep)}')\n",
        "    print(f'cash:    \\t{self.cash}')\n",
        "    print(f'n_shares:\\t{self.n_shares}')\n",
        "    print(f'n_buys:  \\t{self.n_buys}')\n",
        "    print(f'n_sells:\\t{self.n_sells}')\n",
        "    print(f'n_holds:\\t{self.n_holds}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_woJwp4Q_XU"
      },
      "source": [
        "## Data Collection and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1kVKTe2MZP1",
        "outputId": "14e50a71-722b-402a-feaf-154f748d3cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install yfinance\n",
        "!pip install pandas-ta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2La5Q1D5x0sD",
        "outputId": "81632161-99c5-42b8-8adc-0e57c892e449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEXOOvvCzWgf",
        "outputId": "1af70da1-8a11-4158-e26e-d4f8e15af498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "import pandas_ta as pdt\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# GET STOCK DATA\n",
        "stonk = yf.Ticker('CANF')\n",
        "df = stonk.history(start=datetime.now() - timedelta(days=2000), end=datetime.now())\n",
        "df.ta.strategy(\"all\")\n",
        "\n",
        "# Clean data\n",
        "percent_missing = df.isnull().sum() * 100 / len(df)\n",
        "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
        "                                 'percent_missing': percent_missing})\n",
        "for row in missing_value_df.iterrows():\n",
        "  if row[1].percent_missing > 0.1:\n",
        "    df.drop(columns=[row[0]], inplace=True)\n",
        "df = df.dropna()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEBkeuIiRIOM"
      },
      "source": [
        "## Env Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exUuAKQex1iG"
      },
      "source": [
        "env = DeepStockTraderEnv(df)\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXjAo6kYCidU",
        "outputId": "40dcc079-75f1-4bb7-bb1e-068ae755139b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "BUY = 0\n",
        "SELL = 1\n",
        "HOLD = 2\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(random.randint(0, 2))\n",
        "  # print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "env.reset();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlWOn0vsRNoV"
      },
      "source": [
        "## Sample Training Loop\n",
        "\n",
        "*See trainer.py for a pytorch example built by Seth Hamilton*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPF3Vp7zMcGY"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = DeepStockTraderEnv(df)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7c67bqQoip",
        "outputId": "75f80276-ab6b-4bc3-efc3-2dd31ed1b7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the agent\n",
        "model = DQN('MlpPolicy', env, verbose=1).learn(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQj1y20MTqT6",
        "outputId": "9a4779e4-a3cd-4cb5-dd5c-c21e9b5faee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "timestep = 1\n",
        "while True:\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "\n",
        "  # if env.total(timestep) > 10000:\n",
        "  #   pdb.set_trace()\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "  timestep += 1\n",
        "env.render(mode='console')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jteD_gijlnb",
        "outputId": "7d447f0d-2463-4238-8153-ba982a1e4241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "big_gain = np.exp(np.log(1908.20/1000)/(2000/365))\n",
        "big_gain"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
